# Load Scimago journal database
sci_journal <- read.csv("D:/Projekt_COVID/scimagojr_2019.csv",
header = TRUE, sep = ";",stringsAsFactors = FALSE)
sci_journal <- as_tibble(sci_journal)
sci_journal
# Check structure of the data
glimpse(sci_journal)
### Scimago data preparation ###
### Prepare Scimago database for matching (pull Title column)###
sci_journal_1 <- sci_journal %>%
pull(Title)
sci_journal_1
sci_journal_1 <-as_tibble(sci_journal_1)
# Make all the letters in lowercase
sci_journal_1 <- sci_journal_1 %>%
mutate(value =tolower(value))
# Remove punctuation from Title column
sci_journal_2 <- sci_journal_1$value %>%
removePunctuation()
sci_journal_2 <- as_tibble(sci_journal_2)
sci_journal_2
# Rename column to Title
sci_journal_3 <- sci_journal_2 %>%
rename("Title"= "value")
sci_journal_3
# Remove whitespace in Title column
sci_journal_4 <- apply(sci_journal_3,2,function(x)gsub('\\s+', '',x))
sci_journal_4 <- as_tibble(sci_journal_4)
sci_journal_4
# Pull Categories data column from
sci_journal_10 <- sci_journal %>%
pull(Categories)
sci_journal_10 <- as_tibble(sci_journal_10)
sci_journal_10
# Rename value to Categories
sci_journal_12 <- sci_journal_10 %>%
rename("Categories"= "value")
sci_journal_12
# Bind two tibbles (Title and Categories)
sci_journal_13 <- bind_cols(sci_journal_4,sci_journal_12)
sci_journal_13
# Match data
sci_journal_14 <- sci_journal_13$Title[match(f$Title,sci_journal_13$Title)]
sci_journal_14
sci_journal_14 <- as_tibble(sci_journal_14)
sci_journal_14
# Drop missing rows
sci_journal_15 <- sci_journal_14 %>%
drop_na()
sci_journal_15
f
f
sci_journal_14
print(sci_journal_14,n=481)
print(f,n=481)
View(sci_journal_4)
View(sci_journal_4)
View(sci_journal_2)
View(sci_journal_2)
sci_journal_1
print(sci_journal_1,n=30883)
# Make all the letters in lowercase
sci_journal_1 <- sci_journal_1 %>%
mutate(value =tolower(value))
View(sci_journal_1)
View(sci_journal_1)
# Remove all data in parenthesis
h <- apply(sci_journal_1,2,function(x)gsub("\\s*\\([^\\)]+\\)","",x))
# Remove all data after colon
h <- apply(h,2,function(x)gsub(":.*","",x))
# Remove commas
h <- apply(h,2,function(x)gsub(",","",x))
# Remove euqal sign
h <- apply(h,2,function(x)gsub("=","",x))
# Change &amp; to and
h <- apply(h,2,function(x)gsub("&amp;", "and", x))
# Remove whitespace
h <- apply(h,2,function(x)gsub('\\s+', '',x))
# Save as tibble
h <- as_tibble(h)
h
# Remove punctuation from Title column
sci_journal_2 <- h$value %>%
removePunctuation()
sci_journal_2 <- as_tibble(sci_journal_2)
sci_journal_2
# Rename column to Title
sci_journal_3 <- sci_journal_2 %>%
rename("Title"= "value")
sci_journal_3
# Remove whitespace in Title column
sci_journal_4 <- apply(sci_journal_3,2,function(x)gsub('\\s+', '',x))
sci_journal_4 <- as_tibble(sci_journal_4)
sci_journal_4
# Pull Categories data column from
sci_journal_10 <- sci_journal %>%
pull(Categories)
sci_journal_10 <- as_tibble(sci_journal_10)
sci_journal_10
# Rename value to Categories
sci_journal_12 <- sci_journal_10 %>%
rename("Categories"= "value")
sci_journal_12
# Bind two tibbles (Title and Categories)
sci_journal_13 <- bind_cols(sci_journal_4,sci_journal_12)
sci_journal_13
# Match data
sci_journal_14 <- sci_journal_13$Title[match(f$Title,sci_journal_13$Title)]
sci_journal_14
sci_journal_14 <- as_tibble(sci_journal_14)
sci_journal_14
# Drop missing rows
sci_journal_15 <- sci_journal_14 %>%
drop_na()
sci_journal_15
print(sci_journal_15, n=432)
print(sci_journal_14,n=481)
print(sci_journal_1,n=30883)
print(f,n=481)
print(sci_journal_14,n=481)
View(sci_journal_4)
View(sci_journal_4)
# Remove all data in parenthesis
h <- apply(sci_journal_1,2,function(x)gsub("\\s*\\([^\\)]+\\)","",x))
h <- apply(h_1,2,function(x)gsub("-.*","",x))
# Remove all data in parenthesis
h <- apply(sci_journal_1,2,function(x)gsub("\\s*\\([^\\)]+\\)","",x))
h <- apply(h,2,function(x)gsub("-.*","",x))
# Remove all data after colon
h <- apply(h,2,function(x)gsub(":.*","",x))
# Remove commas
h <- apply(h,2,function(x)gsub(",","",x))
# Remove all data in parenthesis
h <- apply(sci_journal_1,2,function(x)gsub("\\s*\\([^\\)]+\\)","",x))
h <- apply(h,2,function(x)gsub("-.*","",x))
# Remove all data after colon
h <- apply(h,2,function(x)gsub(":.*","",x))
# Remove commas
h <- apply(h,2,function(x)gsub(",","",x))
# Remove euqal sign
h <- apply(h,2,function(x)gsub("=","",x))
# Change &amp; to and
h <- apply(h,2,function(x)gsub("&amp;", "and", x))
# Remove whitespace
h <- apply(h,2,function(x)gsub('\\s+', '',x))
# Save as tibble
h <- as_tibble(h)
h
# Remove punctuation from Title column
sci_journal_2 <- h$value %>%
removePunctuation()
sci_journal_2 <- as_tibble(sci_journal_2)
sci_journal_2
# Rename column to Title
sci_journal_3 <- sci_journal_2 %>%
rename("Title"= "value")
sci_journal_3
# Remove whitespace in Title column
sci_journal_4 <- apply(sci_journal_3,2,function(x)gsub('\\s+', '',x))
sci_journal_4 <- as_tibble(sci_journal_4)
sci_journal_4
# Pull Categories data column from
sci_journal_10 <- sci_journal %>%
pull(Categories)
sci_journal_10 <- as_tibble(sci_journal_10)
sci_journal_10
# Rename value to Categories
sci_journal_12 <- sci_journal_10 %>%
rename("Categories"= "value")
sci_journal_12
# Bind two tibbles (Title and Categories)
sci_journal_13 <- bind_cols(sci_journal_4,sci_journal_12)
sci_journal_13
# Match data
sci_journal_14 <- sci_journal_13$Title[match(f$Title,sci_journal_13$Title)]
sci_journal_14
sci_journal_14 <- as_tibble(sci_journal_14)
sci_journal_14
# Drop missing rows
sci_journal_15 <- sci_journal_14 %>%
drop_na()
sci_journal_15
# Load easyPubMed package
library(easyPubMed)
# Retrieve number of all journal articles with abstract regarding COVID 19 from 31th December to July 2020
try({
covid_querry_journal <- get_pubmed_ids("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Journal Article[PT] ")
print(covid_querry_journal$Count)
}, silent = TRUE)
# Retrieve number of review articles regarding COVID 19 from 31th December to July 2020
try({
covid_querry_review <- get_pubmed_ids("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Review[PT] ")
print(covid_querry_review$Count)
}, silent = TRUE)
# Load tidyverse package
library(tidyverse)
# Load easyPubMed package
library(easyPubMed)
# Retrieve number of review articles regarding COVID 19 from 31th December to July 2020
try({
covid_querry_review <- get_pubmed_ids("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Review[PT] ")
print(covid_querry_review$Count)
}, silent = TRUE)
covid_querry_review
library(maps)
library(plyr)
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Review[PT] "
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 1000,
dest_dir = NULL, format = "xml")
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 4000,
dest_dir = NULL, format = "xml")
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Review[PT] "
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 4000,
dest_dir = NULL, format = "xml")
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Review[PT] "
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 4000,
dest_dir = NULL, format = "xml")
readLines(out1[1])[1:30]
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_meta.rds",getKeywords = FALSE,encoding = "UTF8")
# Check structure of the data
str(a)
# Save df as tibble
b <- as_tibble(a)
# Delete unnecessary columns
c <- b %>%
select(-abstract, -keywords,-email)
c
# Pull address column from the rest of the tibble
d <- c %>%
pull(address)
d
s <- gsub("United Kingdom","UK",d)
s <- gsub("United States","USA",d)
s <- gsub("South Korea","S_Korea",d)
s <- gsub("Saudi Arabia","S_Arabia",d)
s <- gsub("New Zealand","N_Zealand",d)
s <- gsub("the Netherlands","Netherlands",d)
s <- gsub("South Africa","S_Africa",d)
# Remove punctuation
e <- gsub("[[:punct:]\n]","",s)
e
# Split data at word boundaries
f <- strsplit(e, " ")
f
# Load world.cities data
data("world.cities")
# Pull country.etc from world.cities
r <- world.cities %>%
pull(country.etc)
# Change names of some countries
r <- gsub("Korea South","S_Korea",r)
r <- gsub("Saudi Arabia","S_Arabia",r)
r <- gsub("New Zealand","N_Zealand",r)
r <- gsub("South Africa","S_Africa",r)
#Match on country in world.countries
CountryList_raw <- (lapply(f, function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))]))
g <- do.call(rbind, lapply(CountryList_raw, as.data.frame))
# Check data structure
str(g)
# Count countries
h <- count(g)
h
# Arrange results in decreasing order
i <- as_tibble(h)
i <- i %>%
arrange(desc(freq))
i
# Cluster data
install.packages("parallel")
library(parallel)
library(parallel)
detectCores()
makeCluster(3)
cluster_1 <- makeCluster(3)
cluster_1
cluster_1
cluster_1
l <- clusterApply(cluster_1,CountryList_raw,do.call(rbind, lapply( as.data.frame )))
CountryList_raw
r
cluster_1 <- makeCluster(3)
l <- clusterApply(cluster_1,f,function(x)x[which(toupper(x) %in% toupper(r))])
l <- clusterApply(cluster_1,f,function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))])
# Load world.cities data
data("world.cities")
l <- clusterApply(cluster_1,f,function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))])
cluster_1 <- makeCluster(3)
clusterEvalQ(cluster_1,world.cities$country.etc)
clusterExport(cluster_1,world.cities$country.etc)
clusterExport(cluster_1,world.cities$country.etc)
detectCores()
cluster_1 <- makeCluster(3)
clusterExport(cluster_1,world.cities$country.etc)
clusterExport(cluster_1,world.cities$country.etc,envir=environment())
clusterExport(cluster_1,world.cities,envir=environment())
clusterExport(cluster_1,c("world.cities","Abasan al-Jadidah"),envir=environment())
clusterExport(cluster_1,c(world.cities,envir=environment("Abasan al-Jadidah")))
clusterExport(cluster_1,world.cities)
clusterExport(cluster_1,"world.cities")
l <- clusterApply(cluster_1,f,function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))])
clusterExport(cluster_1,"world.cities$country.etc")
install.packages("microbenchmark")
library(microbenchmark)
cluster_1 <- makeCluster(3)
clusterExport(cluster_1,"world.cities$country.etc")
clusterExport(cluster_1,"world.cities")
l <- clusterApply(cluster_1,f,function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))])
stopCluster(cluster_1)
l <- do.call(rbind, lapply(CountryList_raw, as.data.frame))
m <- count(l)
m
# Arrange results in decreasing order
m <- as_tibble(m)
m <- m %>%
arrange(desc(freq))
m
microbenchmark(l,CountryList_raw,times = 3)
m
m <- m %>%
rename("Number"= "freq")
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Reviev [PT]"
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 500,
dest_dir = "D:/Projekt_COVID/COVID_Data", format = "xml",
dest_file_prefix = "easyPubMed_data_Countries")
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 500,
dest_dir = "D:/Projekt_COVID/COVID_Data",dest_file_prefix = "easyPubMed_data_Countries",
format = "xml" )
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Reviev[PT]"
out1 <- batch_pubmed_download(pubmed_query_string = ml_query,
dest_dir = "D:/Projekt_COVID/COVID_Data",dest_file_prefix = "easyPubMed_data_Countries",
format = "xml",batch_size = 500, )
out1 <- batch_pubmed_download(pubmed_query_string = ml_query,
dest_dir = NULL,
format = "xml",batch_size = 500, )
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Reviev[PT]"
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 500,
format = "xml")
###sample script - country analysis####
library(tidyverse)
library(easyPubMed)
library(maps)
library(plyr)
library(parallel)
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Reviev[PT]"
library(parallel)
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 500,
format = "xml")
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, dest_dir = NULL, batch_size = 500,
format = "xml")
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 1000,
dest_dir = NULL, format = "xml")
###sample script - Publications matching####
library(tidyverse)
library(easyPubMed)
library(tm)
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Review[PT]"
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 1000,
dest_dir = NULL, format = "xml")
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 1000,
dest_dir = NULL, format = "xml")
Dane_review <- readRDS("D:/Data_R_Meta/Dane_review.rds")
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 1000,
dest_dir = "D:/Projekt_COVID/COVID_Data", format = "xml")
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_meta.rds",getKeywords = FALSE,encoding = "UTF8")
# Check structure of the data
str(a)
# Save df as tibble
b <- as_tibble(a)
# Delete unnecessary columns
c <- b %>%
select(-abstract, -keywords,-email)
c
# Pull address column from the rest of the tibble
d <- c %>%
pull(address)
d
s <- gsub("United Kingdom","UK",d)
s <- gsub("United States","USA",s)
s <- gsub("South Korea","S_Korea",s)
s <- gsub("Saudi Arabia","S_Arabia",s)
s <- gsub("New Zealand","N_Zealand",s)
s <- gsub("the Netherlands","Netherlands",s)
s <- gsub("South Africa","S_Africa",s)
s <- gsub("Czech Republic", "C_Republic",s)
s <- gsub("Costa Rica", "C_Rica",s)
# Remove punctuation
e <- gsub("[[:punct:]\n]","",s)
e
# Split data at word boundaries
f <- strsplit(e, " ")
f
# Load world.cities data
data("world.cities")
# Pull country.etc from world.cities
r <- world.cities %>%
pull(country.etc)
# Change names of some countries
r <- gsub("Korea South","S_Korea",r)
r <- gsub("Saudi Arabia","S_Arabia",r)
r <- gsub("New Zealand","N_Zealand",r)
r <- gsub("South Africa","S_Africa",r)
r <- gsub("Czech Republic", "C_Republic",r)
r <- gsub("Costa Rica", "C_Rica",r)
# Detect number of cores
detectCores()
# Make cluster
cluster_1 <- makeCluster(3)
# Export needed data to every cluster
clusterExport(cluster_1,"world.cities")
# Apply function
l <- clusterApply(cluster_1,f,function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))])
# Stop cluster
stopCluster(cluster_1)
## Tidy outcome data
l <- do.call(rbind, lapply(CountryList_raw, as.data.frame))
## Tidy outcome data
l <- do.call(rbind, lapply(l, as.data.frame))
# Check data structure
str(l)
# Count countries
m <- count(l)
m
# Arrange results in decreasing order
m <- as_tibble(m)
m <- m %>%
arrange(desc(freq))
m
m
l
library(microbenchmark)
microbenchmark(z,a,times=3)
z <- clusterApply(cluster_2,out1,table_articles_byAuth(included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_meta.rds",
getKeywords = FALSE,encoding = "UTF8"))
cluster_2 <- makeCluster(3)
z <- clusterApply(cluster_2,out1,table_articles_byAuth(included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_meta.rds",
getKeywords = FALSE,encoding = "UTF8"))
z <- clusterApply(cluster_2,out1,table_articles_byAuth(pubmed_query_string = ml_query,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_meta.rds",
getKeywords = FALSE,encoding = "UTF8"))
z <- clusterApply(cluster_2,out1,table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_meta.rds",
getKeywords = FALSE,encoding = "UTF8"))
stopCluster(cluster_2)
microbenchmark(z,a,times=3)
###sample script - country analysis####
library(tidyverse)
library(easyPubMed)
library(maps)
library(plyr)
library(parallel)
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]"
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 5000,
dest_dir = "D:/Projekt_COVID/COVID_Data", format = "xml")
readLines(out1[1])[1:30]
readLines(out1[1])[1:30]
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Projekt_COVID/COVID_Data",getKeywords = FALSE,encoding = "UTF8")
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Projekt_COVID/COVID_Data",getKeywords = FALSE,encoding = "UTF8")
readLines(out1[1])[1:30]
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Projekt_COVID",getKeywords = FALSE,encoding = "UTF8")
readLines(out1[1])[1:30]
library(parallel)
library(parallel)
detectCores()
# Load downloaded data
mypath = "D:/Projekt_COVID/COVID_Data"
setwd(mypath)
# Create list of text files
txt_files_ls = list.files(path=mypath, pattern="*.txt")
