e <- as.data.frame(d)
# Install packages "maps" and "plyr"
install.packages("maps")
install.packages("maps")
# Load packages
library(maps)
library(plyr)
# Load word.cities data
data("world.cities")
# Remove punctuation
e <- gsub("[[:punct:]\n]","",e)
# Split data at word boundaries
f <- strsplit(e, " ")
#Match on country in world.countries
CountryList_raw <- (lapply(raw2, function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))]))
g <- do.call(rbind, lapply(CountryList_raw, as.data.frame))
# Check data structure
str(g)
# Count countries
count(g)
# Count countries
count(g)
str
# Check data structure
str(g)
# Count countries
count(g)
# Count countries
h <- count(g)
h
str(h)
read.csv("D:/Projekt_COVID/scimagojr country rank 2019.csv")
j <- read.csv("D:/Projekt_COVID/scimagojr country rank 2019.csv")
j
j <- read.csv("D:/Projekt_COVID/scimagojr_country_rank_2019.csv")
j <- read.csv("D://Projekt_COVID/scimagojr_country_rank_2019.csv")
j <- import("D://Projekt_COVID/scimagojr_country_rank_2019.csv")
j <- read.csv("D://Projekt_COVID/scimagojr_country_rank_2019.csv")
?read.csv
j
install.packages("xlsx")
library("xlsx")
j <- read.xlsx("D:/Projekt_COVID/scimagojr_country_rank_2019.xlsx",1)
j
as.data.frame(j)
as_tibble(j)
as_tibble(h)
h
j
j <- as_tibble(j)
h <- as_tibble(h)
h
j
colnames(h)
h <- rename(h, "X..i.."= "Country")
h <- rename(h, X..i..= "Country")
h <- rename(h, 'X..i..'= "Country")
colnames(h)
h <- rename(h, "freq"'= "Documents")
h <- rename(h, "freq"= "Documents")
colnames(h)
colnames(h)
h
h
###sample script - country analysis####
library(tidyverse)
j <- as_tibble(j)
h <- as_tibble(h)
h
j
colnames(h)
h <- h %>%
rename("freq"= "Documents")
h <- h %>%
rename("Documents"= "freq")
h
h <- h %>%
rename("Country" ="x..i..")
h <- h %>%
rename("Country" ="X..i..")
h
h <- h %>%
as.double("Documents")
h <- h %>%
as.double(unlist("Documents"))
h <- h %>%
unlist("Documents")
h
str(h)
as_tibble(h)
h
# Count countries
h <- count(g)
h
g <- do.call(rbind, lapply(CountryList_raw, as.data.frame))
# Count countries
h <- count(g)
h
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Meta-Analysis[PT] "
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 180,
dest_dir = NULL, format = "xml")
library(easyPubMed)
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Meta-Analysis[PT] "
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 180,
dest_dir = NULL, format = "xml")
readLines(out1[1])[1:30]
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_meta.rds",getKeywords = FALSE,encoding = "UTF8")
# Check structure of the data
str(a)
# Save df as tibble
b <- as_tibble(a)
b
b
# Delete unnecessary columns
c <- b %>%
select(-abstract, -keywords,-email)
c
# Distinct address column from the rest of the tibble
d <- distinct(c,address)
d
j
d
match((d,j))
match(d,j)
v <- read.csv("D:/Projekt_COVID/scimagojr_country_rank_2019.csv")
v
v
# Save data as data frame
e <- as.data.frame(d)
e
e
match(v,e)
grep(v,e)
library(tidyverse)
library(scales)
library(scales)
library(ggpubr)
install.packages("ggpubr")
library(tidyverse)
library(scales)
library(ggpubr)
####UPLOAD####
directory <- "C:/Users/uzytkownik/Dropbox/Badania/Polish medical science network/PubMed/" ####ZMIAN
myfiles_rds <- list.files(path = directory, pattern = "rds",  full.names = TRUE)
d <- tibble()
pb <- txtProgressBar(0, length(myfiles_rds), style = 3)
for(i in 1:length(myfiles_rds)){
p <- read_rds(myfiles_rds[i])
d <- d %>%
bind_rows(p)
setTxtProgressBar(pb, i)
}
unique(v)
unique(e)
d
###sample script - country analysis####
library(tidyverse)
library(easyPubMed)
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Meta-Analysis[PT] "
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 180,
dest_dir = NULL, format = "xml")
readLines(out1[1])[1:30]
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_meta.rds",getKeywords = FALSE,encoding = "UTF8")
# Check structure of the data
str(a)
# Save df as tibble
b <- as_tibble(a)
# Delete unnecessary columns
c <- b %>%
select(-abstract, -keywords,-email)
c
c
v
as_tibble(v)
c
v
v <- as_tibble(v)
v
# Retrieve number of review articles regarding COVID 19 from 31th December to July 2020
try({
covid_querry_review <- get_pubmed_ids("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Review[PT] ")
print(covid_querry_review$Count)
}, silent = TRUE)
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Review[PT] "
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 180,
dest_dir = NULL, format = "xml")
readLines(out1[1])[1:30]
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_review.rds",getKeywords = FALSE,encoding = "UTF8")
# Check structure of the data
str(a)
# Save df as tibble
b <- as_tibble(a)
b
# Delete unnecessary columns
c <- b %>%
select(-abstract, -keywords,-email)
c
# Distinct address column from the rest of the tibble
d <- distinct(c,address)
d
# Save data as data frame
e <- as.data.frame(d)
e
# Load packages
library(maps)
library(plyr)
# Load word.cities data
data("world.cities")
# Remove punctuation
e <- gsub("[[:punct:]\n]","",e)
# Split data at word boundaries
f <- strsplit(e, " ")
#Match on country in world.countries
CountryList_raw <- (lapply(raw2, function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))]))
g <- do.call(rbind, lapply(CountryList_raw, as.data.frame))
# Check data structure
str(g)
# Count countries
h <- count(g)
h
#Match on country in world.countries
CountryList_raw <- (lapply(f, function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))]))
g <- do.call(rbind, lapply(CountryList_raw, as.data.frame))
# Check data structure
str(g)
# Count countries
h <- count(g)
h
i <- as_tibble(h)
i
i <- i %>%
rename("Documents"= "freq")
i <- i %>%
rename("Country" ="X..i..")
i
colnames(i)
i <- i %>%
rename("Documents"= "freq")
i <- i %>%
rename(i,"Documents"= "freq")
i <- i %>%
rename(i,"Documents"= "freq")
colnames(i)
i <- i %>%
rename("Documents" = "freq")
i <- rename(i,"Documents"="freq")
i <- rename(i,"freq"="Documents")
?rename
install.packages("hflights")
library(hflights)
i <- rename(i,"freq"="Documents")
###sample script - country analysis####
library(tidyverse)
library(easyPubMed)
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Review[PT] "
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 180,
dest_dir = NULL, format = "xml")
readLines(out1[1])[1:30]
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/Dane_review.rds",getKeywords = FALSE,encoding = "UTF8")
# Check structure of the data
str(a)
# Save df as tibble
b <- as_tibble(a)
b
# Delete unnecessary columns
c <- b %>%
select(-abstract, -keywords,-email)
c
# Distinct address column from the rest of the tibble
d <- distinct(c,address)
d
# Save data as data frame
e <- as.data.frame(d)
e
# Load packages
library(maps)
library(plyr)
# Load word.cities data
data("world.cities")
# Remove punctuation
e <- gsub("[[:punct:]\n]","",e)
# Split data at word boundaries
f <- strsplit(e, " ")
#Match on country in world.countries
CountryList_raw <- (lapply(f, function(x)x[which(toupper(x) %in% toupper(world.cities$country.etc))]))
g <- do.call(rbind, lapply(CountryList_raw, as.data.frame))
# Check data structure
str(g)
# Count countries
h <- count(g)
h
i <- as_tibble(h)
i
colnames(i)
i <- i %>%
rename("Documents"="freq")
i <- i %>%
rename(Documents="freq")
i <- i %>%
rename('Documents'="freq")
i <- i %>%
rename(i,"Documents"="freq")
library(RISmed)
data(m"myeloma")
data("myeloma")
myeloma
str(myeloma)
covid_review <- EUtilsGet("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]",db="pubmed")
covid_review
covid_review <- EUtilsGet("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[ti] AND 2019/12/31:2020/06/30[DP]",db="pubmed")
covid_review
Affiliation(covid_review)
covid_review <- EUtilsSummary("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[ti] AND 2019/12/31:2020/06/30[DP]",db="pubmed")
covid_review
Affiliation(covid_review)
i
colnames(i)
i <- i %>%
rename("Documents= freq")
i <- i %>%
rename("Documents"= "freq")
i <- i %>%
arrange(desc(freq))
i
i <- i$>$
rename_("Documents" = "freq")
i <- i$>$
rename("Documents" = "freq")
i <- i%>%
rename("Documents" = "freq")
search_topic <- 'COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND
Review[PT] '
search_query <- EUtilsSummary(search_topic, retmax=30000, mindate=2019/12/31,maxdate=2020/06/30)
search_topic <- 'COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND
'
search_query <- EUtilsSummary(search_topic, retmax=30000, mindate=2019/12/31,maxdate=2020/06/30)
search_topic <- 'missed care [TI] OR implicit rationing [TI] OR unfinished care [TI]'
search_query <- EUtilsSummary(search_topic, retmax=1000, mindate=1980,maxdate=2019)
search_query
Affiliation(search_query)
summary(search_query)
search_topic <- 'COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TI]'
search_query <- EUtilsSummary(search_topic, retmax=30000, mindate=2019/12/31,maxdate=2020/06/30)
i
i <- i%>%
rename("Documents" = "freq")
i <- i%>%
rename(Documents = freq)
i
# Retrieve number of all publications regarding COVID 19 from from 31th December to July 2020
try({
covid_querry_all <- get_pubmed_ids("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB]  AND 2019/12/31:2020/06/30[DP]")
print(covid_querry_all$Count)
}, silent = TRUE)
# Retrieve number of all journal articles with abstract regarding COVID 19 from 31th December to July 2020
try({
covid_querry_journal <- get_pubmed_ids("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Journal Article[PT] ")
print(covid_querry_journal$Count)
}, silent = TRUE)
# Retrieve number of review articles regarding COVID 19 from 31th December to July 2020
try({
covid_querry_review <- get_pubmed_ids("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Review[PT] ")
print(covid_querry_review$Count)
}, silent = TRUE)
# Retrieve number of meta_analysis articles regarding COVID 19 from 31th December to July 2020
try({
covid_querry_meta <- get_pubmed_ids("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Meta-Analysis[PT] ")
print(covid_querry_meta$Count)
}, silent = TRUE)
# Retrieve number of guideline articles regarding COVID 19 from 31th December to July 2020
try({
covid_querry_guide <- get_pubmed_ids("COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP] AND
Guideline[PT]")
print(covid_querry_guide$Count)
}, silent = TRUE)
# Count countries
h <- count(g)
h
i <- i %>%
arrange(desc(freq))
i
v
i
colnames(i)
j <- i %>%
rename("Documents"="freq")
j <- i %>%
rename("Documents"="freq")
###sample script - country analysis####
library(tidyverse)
j <- i %>%
rename("Documents"="freq")
colnames(i)
j <- i %>%
rename("Country"="x..i..")
rename(i,"Country"="x..i..")
j <-  rename(i,"Country"="x..i..")
#Download data about Coronavirus COVID 19 meta analysis
ml_query <- "COVID 19 OR novel coronavirus OR
coronavirus Wuhan OR SARS-CoV-2[TIAB] AND 2019/12/31:2020/06/30[DP]
AND Meta-Analysis[PT] "
out1 <- batch_pubmed_download(pubmed_query_string = ml_query, batch_size = 180,
dest_dir = NULL, format = "xml")
readLines(out1[1])[1:30]
# Save downloaded data as df regarding publication authors
a <- table_articles_byAuth(out1,included_authors = "all",
max_chars = 500,autofill = TRUE,dest_file = "D:/Data_R_Meta/journal.rds",getKeywords = FALSE,encoding = "UTF8")
# Check structure of the data
str(a)
# Save df as tibble
b <- as_tibble(a)
# Delete unnecessary columns
c <- b %>%
select(-abstract, -keywords,-email)
c
# Distinct journal column from the rest of the tibble
d <- distinct(c,journal)
d
tolower(d)
d
d <- tolower(d)
d
d <- as_tibble(d)
d
str(d)
c
# Distinct journal column from the rest of the tibble
d <- distinct(c,journal)
d <- tolower(d)
str(d)
d
c
# Distinct journal column from the rest of the tibble
d <- distinct(c,journal)
d
d %>%
mutate(journal = tolower(journal))
# Load Scimago journal database
read.csv("D:/Projekt_COVID/scimagojr 2019 Subject Area - Medicine.csv")
# Load Scimago journal database
read.csv("D:/Projekt_COVID/scimagojr_2019_Subject_Area_Medicine.csv")
# Load Scimago journal database
read.csv("D:/Projekt_COVID/scimagojr_2019_Subject_Area_Medicine.csv", header = TRUE)
# Load Scimago journal database
read.csv("D:/Projekt_COVID/scimagojr_2019_Subject_Area_Medicine.csv", header = TRUE, sep = ";")
# Load Scimago journal database
sci_journal <- read.csv("D:/Projekt_COVID/scimagojr_2019_Subject_Area_Medicine.csv", header = TRUE, sep = ";")
str(sci_journal)
sci_journal <- as_tibble(sci_journal)
sci_journal
glimpse(sci_journal)
sci_journal_1 <- sci_journal_1 %>%
select(-X,-X.1)
sci_journal_1
d
# Load Scimago journal database
sci_journal <- read.csv("D:/Projekt_COVID/scimagojr_2019_Subject_Area_Medicine.csv",
header = TRUE, sep = ";",stringsAsFactors = FALSE)
sci_journal <- as_tibble(sci_journal)
sci_journal
# Check structure of the data
glimpse(sci_journal)
# Prepare Scimago database for matching
sci_journal_1 <- distinct(sci_journal,Title,Categories)
sci_journal_1
sci_journal_1 <- sci_journal_1 %>%
mutate(Title =tolower(Title))
sci_journal_1
d
glimpse(sci_journal_1)
sci_journal_2 <- gsub("[[:punct:]\n]","",sci_journal_1)
sci_journal_2
sci_journal_1
